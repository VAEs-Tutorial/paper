{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of VaDE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockey1006/Variations-on-Variational-Autoencoders/blob/master/Copy_of_VaDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnlSXfgXjlU8",
        "colab_type": "text"
      },
      "source": [
        "# Install and Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t8BTfw9B7Dn",
        "colab_type": "code",
        "outputId": "65c75d05-3f04-4293-b59e-d90130b4657a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        }
      },
      "source": [
        "!git clone https://github.com/slim1017/VaDE.git\n",
        "!pip install --upgrade scikit-learn==0.17.1\n",
        "!pip install --upgrade keras==1.1.0\n",
        "!pip install --upgrade theano==1.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'VaDE'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Total 83 (delta 0), reused 0 (delta 0), pack-reused 83\u001b[K\n",
            "Unpacking objects: 100% (83/83), done.\n",
            "Collecting scikit-learn==0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/1d/9c775f9403565f68aa23f9cef76c817a7115abd7ca1e1c5958a68c49fb6f/scikit-learn-0.17.1.tar.gz (7.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 7.9MB 5.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/17/68/59/e7417b53bb95a8bbbf17f8ab90fa1b8e985141e76de1d110b7\n",
            "Successfully built scikit-learn\n",
            "\u001b[31myellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.17.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mmlxtend 0.14.0 has requirement scikit-learn>=0.18, but you'll have scikit-learn 0.17.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mimbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.17.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mfancyimpute 0.4.2 has requirement scikit-learn>=0.19.1, but you'll have scikit-learn 0.17.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn\n",
            "  Found existing installation: scikit-learn 0.20.2\n",
            "    Uninstalling scikit-learn-0.20.2:\n",
            "      Successfully uninstalled scikit-learn-0.20.2\n",
            "Successfully installed scikit-learn-0.17.1\n",
            "Collecting keras==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/5e/7e64f15f0e5ae65a29c738fc261ce1e0a72d92acfc45f06ef906c6e84bf2/Keras-1.1.0.tar.gz (150kB)\n",
            "\u001b[K    100% |████████████████████████████████| 153kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: theano in /usr/local/lib/python3.6/dist-packages (from keras==1.1.0) (1.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==1.1.0) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from keras==1.1.0) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano->keras==1.1.0) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano->keras==1.1.0) (1.1.0)\n",
            "Building wheels for collected packages: keras\n",
            "  Building wheel for keras (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ae/83/3e/c42ce0672e537640ee706143ebdd1dd691b7693b4ca50f72a8\n",
            "Successfully built keras\n",
            "\u001b[31mtextgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mkapre 0.1.3.1 has requirement keras>=2.0.0, but you'll have keras 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfancyimpute 0.4.2 has requirement keras>=2.0.0, but you'll have keras 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfancyimpute 0.4.2 has requirement scikit-learn>=0.19.1, but you'll have scikit-learn 0.17.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.4\n",
            "    Uninstalling Keras-2.2.4:\n",
            "      Successfully uninstalled Keras-2.2.4\n",
            "Successfully installed keras-1.1.0\n",
            "Collecting theano==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/74/99cc011153b3a566e98729cfee1b8b00af157e2b6047d23ee0871fd5e706/Theano-1.0.0.tar.gz (2.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.9MB 11.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano==1.0.0) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano==1.0.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano==1.0.0) (1.11.0)\n",
            "Building wheels for collected packages: theano\n",
            "  Building wheel for theano (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f3/09/87/48b9871203bd988f59421d9f46e06db5cca884a322d39a5eff\n",
            "Successfully built theano\n",
            "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mkapre 0.1.3.1 has requirement keras>=2.0.0, but you'll have keras 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfancyimpute 0.4.2 has requirement keras>=2.0.0, but you'll have keras 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfancyimpute 0.4.2 has requirement scikit-learn>=0.19.1, but you'll have scikit-learn 0.17.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: theano\n",
            "  Found existing installation: Theano 1.0.4\n",
            "    Uninstalling Theano-1.0.4:\n",
            "      Successfully uninstalled Theano-1.0.4\n",
            "Successfully installed theano-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn1dVJuH66Ke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# overwrite training.py from the VADE \n",
        "!yes | cp -rf VaDE/training.py  ../usr/local/lib/python3.6/dist-packages/keras/engine/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKpQAuQ7uB6z",
        "colab_type": "code",
        "outputId": "687ae974-2c50-4edb-edc5-17ae33572a12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from keras import backend\n",
        "print(backend._BACKEND)\n",
        "from keras import backend as K\n",
        "import importlib;\n",
        "import os\n",
        "def set_keras_backend(backend):\n",
        "    if K.backend() != backend:\n",
        "        os.environ['KERAS_BACKEND'] = backend\n",
        "        importlib.reload(K)\n",
        "        assert K.backend() == backend\n",
        "print (\"Change Keras Backend to Theano\")        \n",
        "set_keras_backend(\"theano\")  \n",
        "from keras import backend; print(backend._BACKEND)\n",
        "\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensorflow\n",
            "Change Keras Backend to Theano\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using Theano backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "theano\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn3-CpX5oLLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input, Dense, Lambda\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import objectives\n",
        "import scipy.io as scio\n",
        "import gzip\n",
        "from six.moves import cPickle\n",
        "import sys\n",
        "import  theano\n",
        "import  theano.tensor as T\n",
        "import math\n",
        "from sklearn import mixture\n",
        "from sklearn.cluster import KMeans\n",
        "from keras.models import model_from_json\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX94v8zkaCpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import tarfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqhJzpxFjq6U",
        "colab_type": "text"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKZAxg4Tibyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_cifar100():\n",
        "    url = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
        "    \n",
        "    local_filename = url.split('/')[-1]\n",
        "\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192): \n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "                    # f.flush()\n",
        "    \n",
        "    tar = tarfile.open(local_filename, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtcuauVxGrwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def floatX(X):\n",
        "    return np.asarray(X, dtype=theano.config.floatX)\n",
        "    \n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.)\n",
        "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "def cluster_acc(Y_pred, Y):\n",
        "  from sklearn.utils.linear_assignment_ import linear_assignment\n",
        "  assert Y_pred.size == Y.size\n",
        "  D = max(Y_pred.max(), Y.max())+1\n",
        "  w = np.zeros((D,D), dtype=np.int64)\n",
        "  for i in range(Y_pred.size):\n",
        "    w[Y_pred[i], Y[i]] += 1\n",
        "  ind = linear_assignment(w.max() - w)\n",
        "  return sum([w[i,j] for i,j in ind])*1.0/Y_pred.size, w\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "def displayImage(image):\n",
        "    displayList=np.array(image).T\n",
        "    im1 = Image.fromarray(displayList)\n",
        "    im1.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMiJwVjoO3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(dataset):\n",
        "\n",
        "    path = 'VaDE/dataset/'+dataset+'/'\n",
        "    if dataset == 'mnist':\n",
        "        path = path + 'mnist.pkl.gz'\n",
        "        if path.endswith(\".gz\"):\n",
        "            f = gzip.open(path, 'rb')\n",
        "        else:\n",
        "            f = open(path, 'rb')\n",
        "    \n",
        "        if sys.version_info < (3,):\n",
        "            (x_train, y_train), (x_test, y_test) = cPickle.load(f)\n",
        "        else:\n",
        "            (x_train, y_train), (x_test, y_test) = cPickle.load(f, encoding=\"bytes\")\n",
        "    \n",
        "        f.close()\n",
        "        x_train = x_train.astype('float32') / 255.\n",
        "        x_test = x_test.astype('float32') / 255.\n",
        "        x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "        x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "        X = np.concatenate((x_train,x_test))\n",
        "        Y = np.concatenate((y_train,y_test))\n",
        "        \n",
        "    if dataset == 'reuters10k':\n",
        "        data=scio.loadmat(path+'reuters10k.mat')\n",
        "        X = data['X']\n",
        "        Y = data['Y'].squeeze()\n",
        "        \n",
        "    if dataset == 'har':\n",
        "        data=scio.loadmat(path+'HAR.mat')\n",
        "        X=data['X']\n",
        "        X=X.astype('float32')\n",
        "        Y=data['Y']-1\n",
        "        X=X[:10200]\n",
        "        Y=Y[:10200]\n",
        "        \n",
        "    if dataset == 'cifer100':\n",
        "        try:\n",
        "            train_data = unpickle('cifar-100-python/train')\n",
        "            test_data = unpickle('cifar-100-python/test')\n",
        "        except:\n",
        "            download_cifar100()\n",
        "            train_data = unpickle('cifar-100-python/train')\n",
        "            test_data = unpickle('cifar-100-python/test')\n",
        "        X_train = train_data[b'data'].astype('float32') / 255.\n",
        "        Y_train = train_data[b'fine_labels']\n",
        "        X_test = test_data[b'data'].astype('float32') / 255.\n",
        "        Y_test = test_data[b'fine_labels']\n",
        "        X = np.concatenate((X_train, X_test))\n",
        "        Y = np.concatenate((Y_train, Y_test))\n",
        "\n",
        "    return X,Y\n",
        "\n",
        "def config_init(dataset):\n",
        "    if dataset == 'mnist':\n",
        "        return 784,3000,10,0.002,0.002,10,0.9,0.9,1,'sigmoid'\n",
        "    if dataset == 'reuters10k':\n",
        "        return 2000,15,4,0.002,0.002,5,0.5,0.5,1,'linear'\n",
        "    if dataset == 'har':\n",
        "        return 561,120,6,0.002,0.00002,10,0.9,0.9,5,'linear'\n",
        "    \n",
        "    # original_dim,epoch,n_centroid,lr_nn,lr_gmm,decay_n,decay_nn,decay_gmm,alpha,datatype\n",
        "    if dataset == 'cifer100':\n",
        "        return 3072,3,100,0.002,0.002,5,0.5,0.5,1,'linear'\n",
        "    \n",
        "\n",
        "        \n",
        "def gmmpara_init():\n",
        "    \n",
        "    theta_init=np.ones(n_centroid)/n_centroid\n",
        "    u_init=np.zeros((latent_dim,n_centroid))\n",
        "    lambda_init=np.ones((latent_dim,n_centroid))\n",
        "    \n",
        "    theta_p=theano.shared(np.asarray(theta_init,dtype=theano.config.floatX),name=\"pi\")\n",
        "    u_p=theano.shared(np.asarray(u_init,dtype=theano.config.floatX),name=\"u\")\n",
        "    lambda_p=theano.shared(np.asarray(lambda_init,dtype=theano.config.floatX),name=\"lambda\")\n",
        "    return theta_p,u_p,lambda_p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okStKgY7GxkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gamma(tempz):\n",
        "    temp_Z=T.transpose(K.repeat(tempz,n_centroid),[0,2,1])\n",
        "    temp_u_tensor3=T.repeat(u_p.dimshuffle('x',0,1),batch_size,axis=0)\n",
        "    temp_lambda_tensor3=T.repeat(lambda_p.dimshuffle('x',0,1),batch_size,axis=0)\n",
        "    temp_theta_tensor3=theta_p.dimshuffle('x','x',0)*T.ones((batch_size,latent_dim,n_centroid))\n",
        "    \n",
        "    temp_p_c_z=K.exp(K.sum((K.log(temp_theta_tensor3)-0.5*K.log(2*math.pi*temp_lambda_tensor3)-\\\n",
        "                       K.square(temp_Z-temp_u_tensor3)/(2*temp_lambda_tensor3)),axis=1))+1e-10\n",
        "    return temp_p_c_z/K.sum(temp_p_c_z,axis=-1,keepdims=True)\n",
        "\n",
        "def vae_loss(x, x_decoded_mean):\n",
        "    Z=T.transpose(K.repeat(z,n_centroid),[0,2,1])\n",
        "    z_mean_t=T.transpose(K.repeat(z_mean,n_centroid),[0,2,1])\n",
        "    z_log_var_t=T.transpose(K.repeat(z_log_var,n_centroid),[0,2,1])\n",
        "    u_tensor3=T.repeat(u_p.dimshuffle('x',0,1),batch_size,axis=0)\n",
        "    lambda_tensor3=T.repeat(lambda_p.dimshuffle('x',0,1),batch_size,axis=0)\n",
        "    theta_tensor3=theta_p.dimshuffle('x','x',0)*T.ones((batch_size,latent_dim,n_centroid))\n",
        "    \n",
        "    p_c_z=K.exp(K.sum((K.log(theta_tensor3)-0.5*K.log(2*math.pi*lambda_tensor3)-\\\n",
        "                       K.square(Z-u_tensor3)/(2*lambda_tensor3)),axis=1))+1e-10\n",
        "\n",
        "    gamma=p_c_z/K.sum(p_c_z,axis=-1,keepdims=True)\n",
        "    gamma_t=K.repeat(gamma,latent_dim)\n",
        "    \n",
        "    if datatype == 'sigmoid':\n",
        "        loss=alpha*original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\\\n",
        "        +K.sum(0.5*gamma_t*(latent_dim*K.log(math.pi*2)+K.log(lambda_tensor3)\\\n",
        "                            +K.exp(z_log_var_t)/lambda_tensor3\\\n",
        "                            +K.square(z_mean_t-u_tensor3)/lambda_tensor3),axis=(1,2))\\\n",
        "        -0.5*K.sum(z_log_var+1,axis=-1)\\\n",
        "        -K.sum(K.log(K.repeat_elements(theta_p.dimshuffle('x',0),batch_size,0))*gamma,axis=-1)\\\n",
        "        +K.sum(K.log(gamma)*gamma,axis=-1)\n",
        "    else:\n",
        "        loss=alpha*original_dim * objectives.mean_squared_error(x, x_decoded_mean)\\\n",
        "        +K.sum(0.5*gamma_t*(latent_dim*K.log(math.pi*2)+K.log(lambda_tensor3)\\\n",
        "                            +K.exp(z_log_var_t)/lambda_tensor3\\\n",
        "                            +K.square(z_mean_t-u_tensor3)/lambda_tensor3),axis=(1,2))\\\n",
        "        -0.5*K.sum(z_log_var+1,axis=-1)\\\n",
        "        -K.sum(K.log(K.repeat_elements(theta_p.dimshuffle('x',0),batch_size,0))*gamma,axis=-1)\\\n",
        "        +K.sum(K.log(gamma)*gamma,axis=-1)\n",
        "        \n",
        "    return loss\n",
        "\n",
        "\n",
        "def load_pretrain_weights(vade,dataset):\n",
        "    ae = model_from_json(open('VaDE/pretrain_weights/ae_'+dataset+'.json').read())\n",
        "    ae.load_weights('VaDE/pretrain_weights/ae_'+dataset+'_weights.h5')\n",
        "    vade.layers[1].set_weights(ae.layers[0].get_weights())\n",
        "    vade.layers[2].set_weights(ae.layers[1].get_weights())\n",
        "    vade.layers[3].set_weights(ae.layers[2].get_weights())\n",
        "    vade.layers[4].set_weights(ae.layers[3].get_weights())\n",
        "    vade.layers[-1].set_weights(ae.layers[-1].get_weights())\n",
        "    vade.layers[-2].set_weights(ae.layers[-2].get_weights())\n",
        "    vade.layers[-3].set_weights(ae.layers[-3].get_weights())\n",
        "    vade.layers[-4].set_weights(ae.layers[-4].get_weights())\n",
        "    sample = sample_output.predict(X,batch_size=batch_size)\n",
        "    if dataset == 'mnist':\n",
        "        g = mixture.GMM(n_components=n_centroid,covariance_type='diag')\n",
        "        g.fit(sample)\n",
        "        u_p.set_value(floatX(g.means_.T))\n",
        "        lambda_p.set_value((floatX(g.covars_.T)))\n",
        "    if dataset == 'reuters10k':\n",
        "        k = KMeans(n_clusters=n_centroid)\n",
        "        k.fit(sample)\n",
        "        u_p.set_value(floatX(k.cluster_centers_.T))\n",
        "    if dataset == 'har':\n",
        "        g = mixture.GMM(n_components=n_centroid,covariance_type='diag',random_state=3)\n",
        "        g.fit(sample)\n",
        "        u_p.set_value(floatX(g.means_.T))\n",
        "        lambda_p.set_value((floatX(g.covars_.T)))\n",
        "    print ('pretrain weights loaded!')\n",
        "    return vade\n",
        "\n",
        "def lr_decay():\n",
        "    if dataset == 'mnist':\n",
        "        adam_nn.lr.set_value(floatX(max(adam_nn.lr.get_value()*decay_nn,0.0002)))\n",
        "        adam_gmm.lr.set_value(floatX(max(adam_gmm.lr.get_value()*decay_gmm,0.0002)))\n",
        "    else:\n",
        "        adam_nn.lr.set_value(floatX(adam_nn.lr.get_value()*decay_nn))\n",
        "        adam_gmm.lr.set_value(floatX(adam_gmm.lr.get_value()*decay_gmm))\n",
        "    print ('lr_nn:%f'%adam_nn.lr.get_value())\n",
        "    print ('lr_gmm:%f'%adam_gmm.lr.get_value())\n",
        "    \n",
        "def epochBegin(epoch):\n",
        "\n",
        "    if epoch % decay_n == 0 and epoch!=0:\n",
        "        lr_decay()\n",
        "    '''\n",
        "    sample = sample_output.predict(X,batch_size=batch_size)\n",
        "    g = mixture.GMM(n_components=n_centroid,covariance_type='diag')\n",
        "    g.fit(sample)\n",
        "    p=g.predict(sample)\n",
        "    acc_g=cluster_acc(p,Y)\n",
        "    \n",
        "    if epoch <1 and ispretrain == False:\n",
        "        u_p.set_value(floatX(g.means_.T))\n",
        "        print ('no pretrain,random init!')\n",
        "    '''\n",
        "    gamma = gamma_output.predict(X,batch_size=batch_size)\n",
        "    acc=cluster_acc(np.argmax(gamma,axis=1),Y)\n",
        "    global accuracy\n",
        "    accuracy+=[acc[0]]\n",
        "    if epoch>0 :\n",
        "        #print ('acc_gmm_on_z:%0.8f'%acc_g[0])\n",
        "        print ('acc_p_c_z:%0.8f'%acc[0])\n",
        "    if epoch==1 and dataset == 'har' and acc[0]<0.77:\n",
        "        print ('=========== HAR dataset:bad init!Please run again! ============')\n",
        "        sys.exit(0)\n",
        "        \n",
        "class EpochBegin(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        epochBegin(epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjGgzV70oVKQ",
        "colab_type": "text"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD9xn2T-GvxW",
        "colab_type": "code",
        "outputId": "4380dfa0-eb1a-4e1e-9b37-8dc4a21946c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset = 'cifer100'\n",
        "db = sys.argv[1]\n",
        "if db in ['mnist','reuters10k','har', 'cifer100']:\n",
        "    dataset = db\n",
        "print ('training on: ' + dataset)\n",
        "ispretrain = False\n",
        "batch_size = 100\n",
        "latent_dim = 10\n",
        "intermediate_dim = [500,500,2000]\n",
        "theano.config.floatX='float32'\n",
        "accuracy=[]\n",
        "X,Y = load_data(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training on: cifer100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucb-S63Gjn6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Parameters\n",
        "epoch = 6\n",
        "n_centroid = 100  # number of categories\n",
        "lr_nn = 0.005\n",
        "lr_gmm = 0.005\n",
        "decay_n = 5\n",
        "decay_nn = 0.5\n",
        "decay_gmm = 0.5\n",
        "alpha = 1\n",
        "datatype = 'linear'\n",
        "original_dim = X[0].size\n",
        "\n",
        "theta_p,u_p,lambda_p = gmmpara_init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdRKHNzrpRyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Input(batch_shape=(batch_size, original_dim))\n",
        "h = Dense(intermediate_dim[0], activation='relu')(x)\n",
        "h = Dense(intermediate_dim[1], activation='relu')(h)\n",
        "h = Dense(intermediate_dim[2], activation='relu')(h)\n",
        "z_mean = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "h_decoded = Dense(intermediate_dim[-1], activation='relu')(z)\n",
        "h_decoded = Dense(intermediate_dim[-2], activation='relu')(h_decoded)\n",
        "h_decoded = Dense(intermediate_dim[-3], activation='relu')(h_decoded)\n",
        "x_decoded_mean = Dense(original_dim, activation=datatype)(h_decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjzwLyWlpXhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Gamma = Lambda(get_gamma, output_shape=(n_centroid,))(z)\n",
        "sample_output = Model(x, z_mean)\n",
        "gamma_output = Model(x,Gamma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZNwylL2pYky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vade = Model(x, x_decoded_mean)\n",
        "\n",
        "if ispretrain == True:\n",
        "    vade = load_pretrain_weights(vade,dataset)\n",
        "\n",
        "adam_nn= Adam(lr=lr_nn,epsilon=1e-4)\n",
        "adam_gmm= Adam(lr=lr_gmm,epsilon=1e-4)\n",
        "\n",
        "vade.compile(optimizer=adam_nn, loss=vae_loss, \n",
        "             add_optimizer=adam_gmm,\n",
        "             add_trainable_weights=[theta_p,u_p,lambda_p])\n",
        "\n",
        "epoch_begin=EpochBegin()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z7QKuRCpZYb",
        "colab_type": "code",
        "outputId": "75e79eec-5705-4ce1-b908-a489bca98af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "vade.fit(X, X,\n",
        "        shuffle=True,\n",
        "        nb_epoch=epoch,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[epoch_begin])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "60000/60000 [==============================] - 107s - loss: 337816.9282   \n",
            "acc_p_c_z:0.01000000\n",
            "Epoch 2/6\n",
            "60000/60000 [==============================] - 431s - loss: 211.1764   \n",
            "acc_p_c_z:0.01000000\n",
            "Epoch 3/6\n",
            "60000/60000 [==============================] - 507s - loss: 204.8619   \n",
            "acc_p_c_z:0.01000000\n",
            "Epoch 4/6\n",
            "60000/60000 [==============================] - 506s - loss: 196.6373   \n",
            "acc_p_c_z:0.01000000\n",
            "Epoch 5/6\n",
            "60000/60000 [==============================] - 505s - loss: 194.2171   \n",
            "lr_nn:0.002500\n",
            "lr_gmm:0.002500\n",
            "acc_p_c_z:0.01000000\n",
            "Epoch 6/6\n",
            "60000/60000 [==============================] - 494s - loss: 192.4294   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f69991224e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAZnCzIg0Kag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
        "# Epoch 1/6\n",
        "# 60000/60000 [==============================] - 101s - loss: 321.7174   \n",
        "# acc_p_c_z:0.01000000\n",
        "# Epoch 2/6\n",
        "# 60000/60000 [==============================] - 373s - loss: 201.2571   \n",
        "# acc_p_c_z:0.01000000\n",
        "# Epoch 3/6\n",
        "# 60000/60000 [==============================] - 422s - loss: 194.2968   \n",
        "# acc_p_c_z:0.01000000\n",
        "# Epoch 4/6\n",
        "# 60000/60000 [==============================] - 430s - loss: 192.9105   \n",
        "# acc_p_c_z:0.01000000\n",
        "# Epoch 5/6\n",
        "# 60000/60000 [==============================] - 427s - loss: 190.1798   \n",
        "# lr_nn:0.002500\n",
        "# lr_gmm:0.002500\n",
        "# acc_p_c_z:0.01000000\n",
        "# Epoch 6/6\n",
        "# 60000/60000 [==============================] - 436s - loss: 184.9435   \n",
        "# <keras.callbacks.History at 0x7fbe77a72e10>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shTcaraKyHjk",
        "colab_type": "text"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCb3XhhlFODr",
        "colab_type": "code",
        "outputId": "d57cc7f0-e3bc-4de9-91b3-55d8391fc936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "p_c_z = Lambda(get_gamma, output_shape=(n_centroid,))(z_mean)\n",
        "sample_output = Model(x, z_mean)\n",
        "p_c_z_output = Model(x, p_c_z)\n",
        "\n",
        "accuracy = cluster_acc(np.argmax(p_c_z_output.predict(X,batch_size=batch_size),axis=1), Y)\n",
        "print('Accuracy: %.2f' % accuracy[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkdiq9pY_dCn",
        "colab_type": "code",
        "outputId": "f1574f39-9dc9-425b-8d12-0714399693cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "accuracy[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[600, 600, 600, ..., 600, 600, 600],\n",
              "       [  0,   0,   0, ...,   0,   0,   0],\n",
              "       [  0,   0,   0, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,   0,   0,   0],\n",
              "       [  0,   0,   0, ...,   0,   0,   0],\n",
              "       [  0,   0,   0, ...,   0,   0,   0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWFQufBIBqAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
